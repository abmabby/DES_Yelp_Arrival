import json
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pytz
from datetime import datetime
from scipy import stats

# ================= ğŸ”¬ å®éªŒå‚æ•°é…ç½® (Configuration) =================
TARGET_BUSINESS_ID = "FEXhWNCMkv22qG04E83Qjg" # CafÃ© Du Monde
TARGET_YEAR = 2015
TARGET_TIMEZONE = 'America/Chicago'

# [å…³é”®ç­–ç•¥]: åªæ‹Ÿåˆé«˜å³°æœŸæ•°æ®ï¼Œä¿è¯ lambda ç›¸å¯¹æ’å®š (Stationary)
PEAK_START_HOUR = 11
PEAK_END_HOUR = 13 

DATA_DIR = './data/'
FIGURE_DIR = './figure/'
# ===================================================================

def calculate_aic(params, dist, data):
    """è®¡ç®— AIC = 2k - 2*ln(L)"""
    log_likelihood = np.sum(dist.logpdf(data, *params))
    k = len(params)
    aic = 2 * k - 2 * log_likelihood
    print(f"    AICè®¡ç®—ç»†èŠ‚: å‚æ•°ä¸ªæ•°k={k}, å¯¹æ•°ä¼¼ç„¶={log_likelihood:.2f}, AIC={aic:.1f}")
    return aic

def load_and_preprocess_data():
    """åŠ è½½å¹¶æ¸…æ´—æ•°æ®"""
    print("="*50)
    print("1. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®")
    print("="*50)
    dates = []
    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
    file_path = os.path.join(DATA_DIR, 'checkin.json')
    if not os.path.exists(file_path):
        print(f"âŒ Error: File not found at {file_path}")
        return pd.DataFrame()

    line_count = 0
    target_line_found = False
    with open(file_path, 'r') as f:
        for line in f:
            line_count += 1
            try:
                data = json.loads(line)
                if data['business_id'] == TARGET_BUSINESS_ID:
                    target_line_found = True
                    raw_dates_str = data['date'].split(',')
                    print(f"    æ‰¾åˆ°ç›®æ ‡å•†å®¶({TARGET_BUSINESS_ID})ï¼ŒåŸå§‹ç­¾åˆ°æ—¶é—´æ•°={len(raw_dates_str)}")
                    raw_dates = [datetime.strptime(d.strip(), "%Y-%m-%d %H:%M:%S") for d in raw_dates_str]
                    # æ—¶åŒºè½¬æ¢ (UTC -> Local)
                    utc = pytz.utc
                    local_tz = pytz.timezone(TARGET_TIMEZONE)
                    for dt in raw_dates:
                        local_dt = utc.localize(dt).astimezone(local_tz)
                        if local_dt.year == TARGET_YEAR:
                            dates.append(local_dt)
                    break
            except Exception as e:
                continue
    
    print(f"    æ‰«æè¡Œæ•°={line_count}, æ˜¯å¦æ‰¾åˆ°ç›®æ ‡å•†å®¶={target_line_found}")
    print(f"    {TARGET_YEAR}å¹´æœ‰æ•ˆç­¾åˆ°æ—¶é—´æ•°={len(dates)}")
    
    df = pd.DataFrame({'dt': dates})
    if not df.empty:
        df['date_str'] = df['dt'].dt.date
        df['hour'] = df['dt'].dt.hour
        df['is_weekend'] = df['dt'].dt.dayofweek.isin([5, 6])
        # æ‰“å°åŸºç¡€ç»Ÿè®¡
        print(f"    æ•°æ®åŸºç¡€ç»Ÿè®¡:")
        print(f"      - æ—¥æœŸèŒƒå›´: {df['dt'].min()} ~ {df['dt'].max()}")
        print(f"      - å·¥ä½œæ—¥ç­¾åˆ°æ•°: {len(df[~df['is_weekend']])}")
        print(f"      - å‘¨æœ«ç­¾åˆ°æ•°: {len(df[df['is_weekend']])}")
        print(f"      - å°æ—¶åˆ†å¸ƒ(å‰5å°æ—¶): {df['hour'].value_counts().head(5)}")
    else:
        print("    âŒ æ— æœ‰æ•ˆæ•°æ®")
    return df

def get_pooled_inter_arrival_times(df_subset, label):
    """è·å–é«˜å³°æœŸåˆå¹¶é—´éš”æ•°æ®ï¼ˆæ–°å¢CVè®¡ç®—ï¼‰"""
    print(f"\n    å¤„ç†{label}é—´éš”æ•°æ®:")
    pooled_intervals = []
    if df_subset.empty: 
        print(f"      âŒ {label}æ•°æ®ä¸ºç©º")
        return np.array([])
    
    grouped = df_subset.groupby('date_str')
    print(f"      æŒ‰æ—¥æœŸåˆ†ç»„æ•°={len(grouped)}")
    
    daily_intervals_count = []
    for idx, (date, group) in enumerate(grouped):
        # 1. ç­›é€‰é«˜å³°æœŸ (Peak Window Slicing)
        peak_data = group[(group['hour'] >= PEAK_START_HOUR) & (group['hour'] < PEAK_END_HOUR)]
        peak_count = len(peak_data)
        
        # å¿…é¡»è‡³å°‘æœ‰2ä¸ªç‚¹æ‰èƒ½ç®—é—´éš”
        if peak_count < 2:
            if idx < 5: # åªæ‰“å°å‰5å¤©çš„ç©ºæ•°æ®
                print(f"        æ—¥æœŸ{date}: é«˜å³°æœŸç­¾åˆ°æ•°={peak_count}ï¼Œè·³è¿‡")
            continue
            
        # 2. æ’åº
        sorted_times = peak_data['dt'].sort_values()
        
        # 3. è®¡ç®—é—´éš” (Diff)ï¼Œè½¬æ¢ä¸ºåˆ†é’Ÿ
        intervals = sorted_times.diff().dropna().dt.total_seconds() / 60.0
        
        # 4. æ¸…æ´—é€»è¾‘
        valid = intervals[(intervals > 0) & (intervals < 60)]
        valid_count = len(valid)
        daily_intervals_count.append(valid_count)
        
        if idx < 5: # åªæ‰“å°å‰5å¤©çš„æœ‰æ•ˆæ•°æ®
            print(f"        æ—¥æœŸ{date}: é«˜å³°æœŸç­¾åˆ°æ•°={peak_count}ï¼Œæœ‰æ•ˆé—´éš”æ•°={valid_count}ï¼Œé—´éš”ç¤ºä¾‹={valid.head(3).round(2).tolist()}")
        
        pooled_intervals.extend(valid.tolist())
    
    # è½¬æ¢ä¸ºæ•°ç»„å¹¶æ‰“å°ç»Ÿè®¡
    intervals_arr = np.array(pooled_intervals)
    print(f"      {label}é—´éš”æ•°æ®ç»Ÿè®¡:")
    print(f"        - æ€»æœ‰æ•ˆé—´éš”æ•°: {len(intervals_arr)}")
    if len(intervals_arr) > 0:
        # æ ¸å¿ƒä¿®æ”¹ï¼šè®¡ç®—å‡å€¼ã€æ ‡å‡†å·®ã€å˜å¼‚ç³»æ•°(CV)
        mean_val = np.mean(intervals_arr)
        std_val = np.std(intervals_arr)
        # å¤„ç†å‡å€¼æ¥è¿‘0çš„æƒ…å†µï¼Œé¿å…é™¤é›¶é”™è¯¯
        if mean_val < 1e-8:
            cv_val = np.nan
        else:
            cv_val = std_val / mean_val  # CV = æ ‡å‡†å·® / å‡å€¼
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–°å¢CVï¼‰
        print(f"        - æè¿°æ€§ç»Ÿè®¡: å‡å€¼={mean_val:.2f}åˆ†é’Ÿ, æ ‡å‡†å·®={std_val:.2f}åˆ†é’Ÿ, å˜å¼‚ç³»æ•°(CV)={cv_val:.2f}")
        print(f"        - ä¸­ä½æ•°={np.median(intervals_arr):.2f}åˆ†é’Ÿ, æœ€å¤§å€¼={np.max(intervals_arr):.2f}åˆ†é’Ÿ")
        print(f"        - å‰10ä¸ªé—´éš”å€¼: {intervals_arr[:10].round(2)}")
    else:
        print(f"        âŒ æ— æœ‰æ•ˆé—´éš”æ•°æ®")
    return intervals_arr

def fit_and_compare_distributions(intervals, label, color, ax):
    """æ‹Ÿåˆåˆ†å¸ƒå¹¶è®¡ç®— AIC, K-Sï¼ˆä»…ä¿ç•™è¿™ä¸¤ä¸ªæŒ‡æ ‡ï¼‰"""
    print("\n" + "="*50)
    print(f"2. æ‹Ÿåˆ{label}åˆ†å¸ƒ")
    print("="*50)
    if len(intervals) < 10:
        print(f"âš ï¸ {label}: æ ·æœ¬é‡ä¸è¶³({len(intervals)})ï¼Œè·³è¿‡æ‹Ÿåˆã€‚")
        return None
    
    fit_results = {}
    
    # 1. ç»˜å›¾ (ç›´æ–¹å›¾)
    sns.histplot(intervals, bins=30, stat='density', alpha=0.3, color=color, label=f'{label} Data', ax=ax)
    x_plot = np.linspace(0, max(intervals), 1000)
    
    # --- å¾…æ‹Ÿåˆçš„åˆ†å¸ƒåˆ—è¡¨ ---
    candidates = [
        ('Exponential', stats.expon, '--'), 
        ('Gamma', stats.gamma, '-'),
        ('Weibull', stats.weibull_min, ':')
    ]
    
    # åˆå§‹åŒ–æœ€ä¼˜AIC
    min_aic = float('inf')
    best_dist_name = ""
    
    # 2. éå†æ‹Ÿåˆæ¯ä¸ªåˆ†å¸ƒ
    for name, dist, style in candidates:
        print(f"\n    ğŸ‘‰ æ‹Ÿåˆ{name}åˆ†å¸ƒ:")
        # A. æ‹Ÿåˆå‚æ•° (floc=0å›ºå®šä½ç½®å‚æ•°ä¸º0)
        params = dist.fit(intervals, floc=0)
        print(f"      æ‹Ÿåˆå‚æ•°: {params} (floc=0å›ºå®š)")
        
        # B. è®¡ç®—AIC
        aic = calculate_aic(params, dist, intervals)
        
        # C. åˆ†å¸ƒå‚æ•°è§£æä¸æ‰“å°
        if name == 'Exponential':
            scale = params[1]
            lambda_per_min = 1.0 / scale
            lambda_per_hour = lambda_per_min * 60
            param_str = f"Î»={lambda_per_min:.3f}/min (Î»={lambda_per_hour:.1f}/hr)"
            print(f"      æŒ‡æ•°åˆ†å¸ƒlambda: {param_str}")
        elif name == 'Gamma':
            shape, scale = params[0], params[2]
            param_str = f"Î±={shape:.2f}, Î²={scale:.2f}"
            print(f"      Gammaåˆ†å¸ƒå‚æ•°: å½¢çŠ¶Î±={shape:.2f}, å°ºåº¦Î²={scale:.2f}")
        elif name == 'Weibull':
            shape, scale = params[0], params[2]
            param_str = f"c={shape:.2f}, Î²={scale:.2f}"
            print(f"      Weibullåˆ†å¸ƒå‚æ•°: å½¢çŠ¶c={shape:.2f}, å°ºåº¦Î²={scale:.2f}")
        else:
            param_str = ""
        
        # D. K-S Test
        print(f"      è®¡ç®—K-Sæ£€éªŒ...")
        ks_stat, ks_p = stats.kstest(intervals, dist.name, args=params)
        print(f"      K-Sæ£€éªŒç»“æœ: ç»Ÿè®¡é‡={ks_stat:.4f}, på€¼={ks_p:.2e}")
        
        # G. è®¡ç®—ç†è®ºPDFå€¼
        pdf_vals = dist.pdf(x_plot, *params)
        
        # ä¿å­˜ç»“æœï¼ˆä»…ä¿ç•™AICå’ŒKSï¼‰
        fit_results[name] = {
            'params': params,
            'aic': aic,
            'ks_stat': ks_stat, 
            'ks_p': ks_p
        }
        
        # è®°å½•æœ€ä¼˜AIC
        if aic < min_aic:
            min_aic = aic
            best_dist_name = name
        
        # H. ç»˜å›¾
        if name == 'Exponential':
            line_color = 'black'
        elif name == 'Gamma':
            line_color = color
        elif name == 'Weibull':
            line_color = 'green'
        ax.plot(x_plot, pdf_vals, linestyle=style, 
                color=line_color, linewidth=2, label=f'{name} ({param_str})')
    
    # 3. è®¡ç®—Î”AICå¹¶æ‰“å°
    print(f"\n    ğŸ“Š {label}æ‹Ÿåˆç»“æœæ±‡æ€»:")
    for dist_name, metrics in fit_results.items():
        delta_aic = metrics['aic'] - min_aic
        fit_results[dist_name]['delta_aic'] = delta_aic
        print(f"      {dist_name}:")
        print(f"        - AIC={metrics['aic']:.1f}, Î”AIC={delta_aic:.1f}")
        print(f"        - K-S p={metrics['ks_p']:.2e}")
    
    print(f"      âœ… æœ€ä¼˜åˆ†å¸ƒ(AICæœ€å°): {best_dist_name} (AIC={min_aic:.1f})")
    
    # 4. ç”Ÿæˆå›¾ä¸Šçš„ç»Ÿè®¡æ–‡æœ¬ (ä»…ä¿ç•™AICå’ŒKS)
    stats_text = f"Sample N: {len(intervals)}\nMean: {np.mean(intervals):.2f} min\n"
    # æ–°å¢ï¼šåœ¨å›¾ç‰‡æ–‡æœ¬ä¸­ä¹ŸåŠ å…¥CV
    if len(intervals) > 0:
        mean_val = np.mean(intervals)
        std_val = np.std(intervals)
        cv_val = std_val / mean_val if mean_val > 1e-8 else np.nan
        stats_text += f"CV: {cv_val:.2f}\n"
    stats_text += "-" * 25 + "\n"
    for dist_name, metrics in fit_results.items():
        delta_aic = metrics['delta_aic']
        stats_text += f"[{dist_name}]\n"
        stats_text += f" AIC: {metrics['aic']:.1f} (Î”={delta_aic:.1f})\n"
        stats_text += f" K-S p: {metrics['ks_p']:.2e}\n"
    
    # å›¾ç‰‡æ–‡æœ¬ä½ç½®è°ƒæ•´
    ax.text(0.55, 0.15, stats_text, transform=ax.transAxes, fontsize=8,
            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.9), verticalalignment='top')

    # å›¾ç‰‡æ ‡é¢˜/æ ‡ç­¾
    ax.set_title(f"{label} Arrival Intervals ({PEAK_START_HOUR}:00-{PEAK_END_HOUR}:00)", fontsize=14)
    ax.set_xlabel("Inter-arrival Time (minutes)", fontsize=12)
    ax.set_ylabel("Probability Density", fontsize=12)
    ax.legend(loc='upper right', fontsize=10)
    ax.set_xlim(0, 40)
    ax.grid(alpha=0.3)
    
    return fit_results

def step3_ultimate_fitting():
    print("="*60)
    print("å¼€å§‹æ‰§è¡Œ [Module 3] åˆ†å¸ƒæ‹Ÿåˆ (ä»…KS & AIC + CVç»Ÿè®¡)")
    print("="*60)
    os.makedirs(FIGURE_DIR, exist_ok=True)
    
    # 1. åŠ è½½æ•°æ®
    df = load_and_preprocess_data()
    if df.empty: 
        print("âŒ æ— æ•°æ®ï¼Œç»ˆæ­¢ç¨‹åº")
        return

    # 2. åˆ†å‰²å·¥ä½œæ—¥/å‘¨æœ«å¹¶æå–é—´éš”
    print("\n" + "="*50)
    print("æå–é«˜å³°æœŸåˆ°è¾¾é—´éš”æ•°æ® (å«CVè®¡ç®—)")
    print("="*50)
    intervals_wd = get_pooled_inter_arrival_times(df[~df['is_weekend']], "å·¥ä½œæ—¥")
    intervals_we = get_pooled_inter_arrival_times(df[df['is_weekend']], "å‘¨æœ«")
    
    # 3. ç»˜å›¾æ‹Ÿåˆ
    fig, axes = plt.subplots(1, 2, figsize=(18, 8))
    
    print("\n--- æ‹Ÿåˆå·¥ä½œæ—¥åˆ†å¸ƒ ---")
    res_wd = fit_and_compare_distributions(intervals_wd, "Weekday", "blue", axes[0])
    
    print("\n--- æ‹Ÿåˆå‘¨æœ«åˆ†å¸ƒ ---")
    res_we = fit_and_compare_distributions(intervals_we, "Weekend", "red", axes[1])
    
    # ä¿å­˜å›¾ç‰‡ï¼ˆä¿®æ”¹å‘½åä¸ºdistribution_fitting_clearï¼‰
    plt.tight_layout()
    save_path = os.path.join(FIGURE_DIR, 'distribution_fitting_clear.png')
    plt.savefig(save_path, dpi=300)
    print(f"\nâœ… å›¾ç‰‡å·²ä¿å­˜è‡³: {save_path}")
    
    # 4. æ‰“å°æœ€ç»ˆæ±‡æ€»ï¼ˆä»…ä¿ç•™AICå’ŒKSï¼‰
    print("\n" + "="*60)
    print("æœ€ç»ˆæ‹Ÿåˆç»“æœæ±‡æ€» (æ§åˆ¶å°ç‰ˆ)")
    print("="*60)
    for label_cn, label_en, res in [("å·¥ä½œæ—¥", "Weekday", res_wd), ("å‘¨æœ«", "Weekend", res_we)]:
        print(f"\nğŸ“ˆ {label_cn} ({label_en}):")
        if res is None:
            print("  âŒ æ— æ‹Ÿåˆç»“æœ")
            continue
        # è¡¥å……æ‰“å°CVåˆ°æœ€ç»ˆæ±‡æ€»
        intervals = intervals_wd if label_cn == "å·¥ä½œæ—¥" else intervals_we
        if len(intervals) > 0:
            mean_val = np.mean(intervals)
            std_val = np.std(intervals)
            cv_val = std_val / mean_val if mean_val > 1e-8 else np.nan
            print(f"  ğŸ“Š åŸºç¡€ç»Ÿè®¡è¡¥å……: å˜å¼‚ç³»æ•°(CV)={cv_val:.2f}")
        for dist, metrics in res.items():
            delta_aic = metrics.get('delta_aic', 0)
            print(f"  ğŸ“Š {dist}:")
            print(f"    - å‚æ•°: {metrics['params']}")
            print(f"    - AIC={metrics['aic']:.1f}, Î”AIC={delta_aic:.1f}")
            print(f"    - K-Sæ£€éªŒ: ç»Ÿè®¡é‡={metrics['ks_stat']:.4f}, på€¼={metrics['ks_p']:.2e}")
        # æ ‡æ³¨æœ€ä¼˜åˆ†å¸ƒ
        best_dist = min(res.keys(), key=lambda k: res[k]['aic'])
        print(f"  âœ… æœ€ä¼˜åˆ†å¸ƒ: {best_dist} (AICæœ€å°={res[best_dist]['aic']:.1f})")

if __name__ == "__main__":
    step3_ultimate_fitting()