import json
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pytz
from datetime import datetime
from scipy import stats

# ================= ğŸ”¬ å®éªŒå‚æ•°é…ç½® (Configuration) =================
TARGET_BUSINESS_ID = "FEXhWNCMkv22qG04E83Qjg" # CafÃ© Du Monde
TARGET_YEAR = 2015
TARGET_TIMEZONE = 'America/Chicago'

# [å…³é”®ç­–ç•¥]: åªæ‹Ÿåˆé«˜å³°æœŸæ•°æ®ï¼Œä¿è¯ lambda ç›¸å¯¹æ’å®š (Stationary)
PEAK_START_HOUR = 11
PEAK_END_HOUR = 13 

DATA_DIR = './data/'
FIGURE_DIR = './figure/'
# ===================================================================

def calculate_aic(params, dist, data):
    """è®¡ç®— AIC = 2k - 2*ln(L)"""
    log_likelihood = np.sum(dist.logpdf(data, *params))
    k = len(params)
    aic = 2 * k - 2 * log_likelihood
    print(f"    AICè®¡ç®—ç»†èŠ‚: å‚æ•°ä¸ªæ•°k={k}, å¯¹æ•°ä¼¼ç„¶={log_likelihood:.2f}, AIC={aic:.1f}")
    return aic

import numpy as np
from scipy import stats

def calculate_chi_square(data, dist, params, n_bins=None):
    """
    ä¼˜åŒ–ç‰ˆå¡æ–¹æ£€éªŒï¼šè§£å†³åˆ†æ¡¶ä¸åˆç†+é¢‘æ•°æ€»å’Œä¸ä¸€è‡´å¯¼è‡´çš„æŠ¥é”™é—®é¢˜
    å‚æ•°ï¼š
        data: åˆ°è¾¾é—´éš”æ•°æ®ï¼ˆä¸€ç»´æ•°ç»„ï¼‰
        dist: scipy.statsçš„åˆ†å¸ƒå¯¹è±¡ï¼ˆå¦‚stats.exponã€stats.gammaï¼‰
        params: æœ€å¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°çš„åˆ†å¸ƒå‚æ•°ï¼ˆå…ƒç»„ï¼‰
        n_bins: åˆå§‹åˆ†æ¡¶æ•°ï¼ˆé»˜è®¤æŒ‰æ ·æœ¬é‡è‡ªåŠ¨è®¡ç®—ï¼š300æ ·æœ¬â‰ˆ10æ¡¶ï¼‰
    """
    # 1. è‡ªåŠ¨ç¡®å®šåˆç†åˆ†æ¡¶æ•°ï¼ˆ300æ ·æœ¬å»ºè®®n_bins=10ï¼Œä¿è¯æ¯æ¡¶æœŸæœ›â‰¥5ï¼‰
    if n_bins is None:
        n_bins = min(10, len(data)//30)  # 300æ ·æœ¬â†’10æ¡¶ï¼Œæ¯æ¡¶è‡³å°‘30ä¸ªè§‚å¯Ÿå€¼
    
    # 2. æ”¹ç”¨ã€Œç­‰æ¦‚ç‡åˆ†æ¡¶ã€ï¼ˆé€‚åˆåæ€åˆ†å¸ƒï¼Œä¿è¯æ¯ä¸ªæ¡¶çš„æœŸæœ›é¢‘æ•°æ¥è¿‘ï¼‰
    # æ­¥éª¤ï¼šå…ˆè®¡ç®—æ‹Ÿåˆåˆ†å¸ƒçš„åˆ†ä½æ•°ï¼Œä½œä¸ºåˆ†æ¡¶è¾¹ç•Œ
    bin_edges = dist.ppf(np.linspace(0, 1, n_bins+1), *params)
    # ä¿®æ­£è¾¹ç•Œï¼šé¿å…å› æµ®ç‚¹è¯¯å·®å¯¼è‡´è¾¹ç•Œè¶…å‡ºæ•°æ®èŒƒå›´
    bin_edges[0] = np.min(data) - 1e-10
    bin_edges[-1] = np.max(data) + 1e-10
    
    # 3. è®¡ç®—è§‚å¯Ÿé¢‘æ•°ï¼ˆæŒ‰ç­‰æ¦‚ç‡åˆ†æ¡¶ï¼‰
    observed, _ = np.histogram(data, bins=bin_edges)
    print(f"    å¡æ–¹æ£€éªŒåˆ†æ¡¶: å…±{n_bins}æ¡¶ï¼Œæ¡¶è¾¹ç•Œ={bin_edges[:5].round(2)}...(åç•¥)")
    print(f"    è§‚å¯Ÿé¢‘æ•°(å‰5æ¡¶): {observed[:5]}")
    
    # 4. è®¡ç®—æœŸæœ›é¢‘æ•°ï¼ˆåŸºäºæ‹Ÿåˆåˆ†å¸ƒçš„çœŸå®æ¦‚ç‡ï¼Œä¸åšå½’ä¸€åŒ–ï¼‰
    N = len(data)
    expected = []
    for i in range(len(observed)):
        lower, upper = bin_edges[i], bin_edges[i+1]
        prob = dist.cdf(upper, *params) - dist.cdf(lower, *params)
        exp_val = prob * N
        expected.append(exp_val)
    expected = np.array(expected)
    print(f"    æœŸæœ›é¢‘æ•°(å‰5æ¡¶): {expected[:5].round(2)}")
    
    # 5. åˆå¹¶ä½æœŸæœ›æ¡¶ï¼ˆä¿è¯æ‰€æœ‰æœŸæœ›é¢‘æ•°â‰¥5ï¼‰
    merged_observed = []
    merged_expected = []
    temp_obs, temp_exp = 0, 0
    for obs, exp in zip(observed, expected):
        temp_obs += obs
        temp_exp += exp
        # å½“ç´¯è®¡æœŸæœ›â‰¥5æ—¶ï¼Œä¿ç•™è¯¥æ¡¶ï¼›å¦åˆ™ç»§ç»­åˆå¹¶
        if temp_exp >= 5:
            merged_observed.append(temp_obs)
            merged_expected.append(temp_exp)
            temp_obs, temp_exp = 0, 0
    # å¤„ç†æœ€åä¸€ä¸ªæœªåˆå¹¶çš„æ¡¶ï¼ˆå³ä½¿<5ä¹Ÿä¿ç•™ï¼Œé¿å…ä¸¢å¤±æ•°æ®ï¼‰
    if temp_obs > 0:
        if merged_observed:  # è‹¥å·²æœ‰åˆå¹¶æ¡¶ï¼Œè¿½åŠ åˆ°æœ€åä¸€ä¸ª
            merged_observed[-1] += temp_obs
            merged_expected[-1] += temp_exp
        else:  # æç«¯æƒ…å†µï¼šæ‰€æœ‰æ¡¶éƒ½<5ï¼Œç›´æ¥ä¿ç•™
            merged_observed.append(temp_obs)
            merged_expected.append(temp_exp)
    merged_observed = np.array(merged_observed)
    merged_expected = np.array(merged_expected)
    print(f"    åˆå¹¶ä½æœŸæœ›æ¡¶å: å‰©ä½™{len(merged_observed)}æ¡¶")
    print(f"    åˆå¹¶åè§‚å¯Ÿé¢‘æ•°: {merged_observed[:5]}")
    print(f"    åˆå¹¶åæœŸæœ›é¢‘æ•°: {merged_expected[:5].round(2)}")
    
    # ========== å…³é”®ä¿®æ­£ï¼šæ¸©å’Œå½’ä¸€åŒ–ï¼Œæ ¡å‡†æ€»å’Œ ==========
    obs_sum = np.sum(merged_observed)
    exp_sum = np.sum(merged_expected)
    # è®¡ç®—æ ¡å‡†å› å­ï¼ˆä»…ä¿®æ­£æ€»å’Œï¼Œä¸æ”¹å˜ç›¸å¯¹æ¯”ä¾‹ï¼‰
    calibration_factor = obs_sum / exp_sum
    merged_expected = merged_expected * calibration_factor
    # æ ¡å‡†åå†æ¬¡æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰æœŸæœ›é¢‘æ•°â‰¥5ï¼ˆé¿å…æ ¡å‡†åå‡ºç°ä½æœŸæœ›ï¼‰
    merged_expected[merged_expected < 5] = 5.0  # æç«¯æƒ…å†µå…œåº•
    print(f"    æ ¡å‡†åï¼šè§‚å¯Ÿé¢‘æ•°æ€»å’Œ={obs_sum}, æœŸæœ›é¢‘æ•°æ€»å’Œ={np.sum(merged_expected):.2f}")
    
    # 6. å¤„ç†æç«¯æƒ…å†µï¼ˆé˜²æ­¢é™¤é›¶ï¼Œä»…åŠ æå°å€¼ï¼‰
    merged_expected[merged_expected == 0] = 1e-10
    
    # 7. è®¡ç®—å¡æ–¹ç»Ÿè®¡é‡å’ŒPå€¼ï¼ˆç”¨åˆå¹¶åçš„é¢‘æ•°ï¼‰
    chi2_stat, p_val = stats.chisquare(f_obs=merged_observed, f_exp=merged_expected)
    
    print(f"    å¡æ–¹æ£€éªŒç»“æœ: ç»Ÿè®¡é‡={chi2_stat:.2f}, på€¼={p_val:.4f}")
    return chi2_stat, p_val

def calculate_square_error(data, dist, params, n_bins=50):
    """
    è®¡ç®—å¹³æ–¹è¯¯å·® (Square Error) - Arena Input Analyzer çš„æ ¸å¿ƒæŒ‡æ ‡
    åŸç†: Sum((Density_Obs - Density_Theo)^2)
    """
    # 1. è·å–ç»éªŒå¯†åº¦ (ç›´æ–¹å›¾)
    hist_vals, bin_edges = np.histogram(data, bins=n_bins, density=True)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    
    # 2. è·å–ç†è®ºå¯†åº¦ (PDF)
    pdf_vals = dist.pdf(bin_centers, *params)
    
    # 3. è®¡ç®—è¯¯å·®å¹³æ–¹å’Œ
    sq_error = np.sum((hist_vals - pdf_vals) ** 2)
    print(f"    å¹³æ–¹è¯¯å·®è®¡ç®—: åˆ†æ¡¶æ•°={n_bins}, è¯¯å·®å¹³æ–¹å’Œ={sq_error:.6f}")
    return sq_error

def load_and_preprocess_data():
    """åŠ è½½å¹¶æ¸…æ´—æ•°æ®"""
    print("="*50)
    print("1. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®")
    print("="*50)
    dates = []
    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
    file_path = os.path.join(DATA_DIR, 'checkin.json')
    if not os.path.exists(file_path):
        print(f"âŒ Error: File not found at {file_path}")
        return pd.DataFrame()

    line_count = 0
    target_line_found = False
    with open(file_path, 'r') as f:
        for line in f:
            line_count += 1
            try:
                data = json.loads(line)
                if data['business_id'] == TARGET_BUSINESS_ID:
                    target_line_found = True
                    raw_dates_str = data['date'].split(',')
                    print(f"    æ‰¾åˆ°ç›®æ ‡å•†å®¶({TARGET_BUSINESS_ID})ï¼ŒåŸå§‹ç­¾åˆ°æ—¶é—´æ•°={len(raw_dates_str)}")
                    raw_dates = [datetime.strptime(d.strip(), "%Y-%m-%d %H:%M:%S") for d in raw_dates_str]
                    # æ—¶åŒºè½¬æ¢ (UTC -> Local)
                    utc = pytz.utc
                    local_tz = pytz.timezone(TARGET_TIMEZONE)
                    for dt in raw_dates:
                        local_dt = utc.localize(dt).astimezone(local_tz)
                        if local_dt.year == TARGET_YEAR:
                            dates.append(local_dt)
                    break
            except Exception as e:
                continue
    
    print(f"    æ‰«æè¡Œæ•°={line_count}, æ˜¯å¦æ‰¾åˆ°ç›®æ ‡å•†å®¶={target_line_found}")
    print(f"    {TARGET_YEAR}å¹´æœ‰æ•ˆç­¾åˆ°æ—¶é—´æ•°={len(dates)}")
    
    df = pd.DataFrame({'dt': dates})
    if not df.empty:
        df['date_str'] = df['dt'].dt.date
        df['hour'] = df['dt'].dt.hour
        df['is_weekend'] = df['dt'].dt.dayofweek.isin([5, 6])
        # æ‰“å°åŸºç¡€ç»Ÿè®¡
        print(f"    æ•°æ®åŸºç¡€ç»Ÿè®¡:")
        print(f"      - æ—¥æœŸèŒƒå›´: {df['dt'].min()} ~ {df['dt'].max()}")
        print(f"      - å·¥ä½œæ—¥ç­¾åˆ°æ•°: {len(df[~df['is_weekend']])}")
        print(f"      - å‘¨æœ«ç­¾åˆ°æ•°: {len(df[df['is_weekend']])}")
        print(f"      - å°æ—¶åˆ†å¸ƒ(å‰5å°æ—¶): {df['hour'].value_counts().head(5)}")
    else:
        print("    âŒ æ— æœ‰æ•ˆæ•°æ®")
    return df

def get_pooled_inter_arrival_times(df_subset, label):
    """è·å–é«˜å³°æœŸåˆå¹¶é—´éš”æ•°æ®"""
    print(f"\n    å¤„ç†{label}é—´éš”æ•°æ®:")
    pooled_intervals = []
    if df_subset.empty: 
        print(f"      âŒ {label}æ•°æ®ä¸ºç©º")
        return np.array([])
    
    grouped = df_subset.groupby('date_str')
    print(f"      æŒ‰æ—¥æœŸåˆ†ç»„æ•°={len(grouped)}")
    
    daily_intervals_count = []
    for idx, (date, group) in enumerate(grouped):
        # 1. ç­›é€‰é«˜å³°æœŸ (Peak Window Slicing)
        peak_data = group[(group['hour'] >= PEAK_START_HOUR) & (group['hour'] < PEAK_END_HOUR)]
        peak_count = len(peak_data)
        
        # å¿…é¡»è‡³å°‘æœ‰2ä¸ªç‚¹æ‰èƒ½ç®—é—´éš”
        if peak_count < 2:
            if idx < 5: # åªæ‰“å°å‰5å¤©çš„ç©ºæ•°æ®
                print(f"        æ—¥æœŸ{date}: é«˜å³°æœŸç­¾åˆ°æ•°={peak_count}ï¼Œè·³è¿‡")
            continue
            
        # 2. æ’åº
        sorted_times = peak_data['dt'].sort_values()
        
        # 3. è®¡ç®—é—´éš” (Diff)ï¼Œè½¬æ¢ä¸ºåˆ†é’Ÿ
        intervals = sorted_times.diff().dropna().dt.total_seconds() / 60.0
        
        # 4. æ¸…æ´—é€»è¾‘
        valid = intervals[(intervals > 0) & (intervals < 60)]
        valid_count = len(valid)
        daily_intervals_count.append(valid_count)
        
        if idx < 5: # åªæ‰“å°å‰5å¤©çš„æœ‰æ•ˆæ•°æ®
            print(f"        æ—¥æœŸ{date}: é«˜å³°æœŸç­¾åˆ°æ•°={peak_count}ï¼Œæœ‰æ•ˆé—´éš”æ•°={valid_count}ï¼Œé—´éš”ç¤ºä¾‹={valid.head(3).round(2).tolist()}")
        
        pooled_intervals.extend(valid.tolist())
    
    # è½¬æ¢ä¸ºæ•°ç»„å¹¶æ‰“å°ç»Ÿè®¡
    intervals_arr = np.array(pooled_intervals)
    print(f"      {label}é—´éš”æ•°æ®ç»Ÿè®¡:")
    print(f"        - æ€»æœ‰æ•ˆé—´éš”æ•°: {len(intervals_arr)}")
    if len(intervals_arr) > 0:
        print(f"        - æè¿°æ€§ç»Ÿè®¡: å‡å€¼={np.mean(intervals_arr):.2f}åˆ†é’Ÿ, æ ‡å‡†å·®={np.std(intervals_arr):.2f}åˆ†é’Ÿ")
        print(f"        - ä¸­ä½æ•°={np.median(intervals_arr):.2f}åˆ†é’Ÿ, æœ€å¤§å€¼={np.max(intervals_arr):.2f}åˆ†é’Ÿ")
        print(f"        - å‰10ä¸ªé—´éš”å€¼: {intervals_arr[:10].round(2)}")
    else:
        print(f"        âŒ æ— æœ‰æ•ˆé—´éš”æ•°æ®")
    return intervals_arr

def fit_and_compare_distributions(intervals, label, color, ax):
    """æ‹Ÿåˆåˆ†å¸ƒå¹¶è®¡ç®— AIC, K-S, Chi-Square, Square Errorï¼ˆå›¾ç‰‡çº¯è‹±æ–‡ï¼‰"""
    print("\n" + "="*50)
    print(f"2. æ‹Ÿåˆ{label}åˆ†å¸ƒ")
    print("="*50)
    if len(intervals) < 10:
        print(f"âš ï¸ {label}: æ ·æœ¬é‡ä¸è¶³({len(intervals)})ï¼Œè·³è¿‡æ‹Ÿåˆã€‚")
        return None
    
    fit_results = {}
    
    # 1. ç»˜å›¾ (ç›´æ–¹å›¾)
    sns.histplot(intervals, bins=30, stat='density', alpha=0.3, color=color, label=f'{label} Data', ax=ax)
    x_plot = np.linspace(0, max(intervals), 1000)
    
    # --- å¾…æ‹Ÿåˆçš„åˆ†å¸ƒåˆ—è¡¨ (æ–°å¢Weibull) ---
    candidates = [
        ('Exponential', stats.expon, '--'), 
        ('Gamma', stats.gamma, '-'),
        ('Weibull', stats.weibull_min, ':')  # æ–°å¢Weibullåˆ†å¸ƒï¼Œçº¿å‹ä¸ºç‚¹çº¿
    ]
    
    # åˆå§‹åŒ–æœ€ä¼˜AIC
    min_aic = float('inf')
    best_dist_name = ""
    
    # 2. éå†æ‹Ÿåˆæ¯ä¸ªåˆ†å¸ƒ
    for name, dist, style in candidates:
        print(f"\n    ğŸ‘‰ æ‹Ÿåˆ{name}åˆ†å¸ƒ:")
        # A. æ‹Ÿåˆå‚æ•° (floc=0å›ºå®šä½ç½®å‚æ•°ä¸º0)
        params = dist.fit(intervals, floc=0)
        print(f"      æ‹Ÿåˆå‚æ•°: {params} (floc=0å›ºå®š)")
        
        # B. è®¡ç®—AIC
        aic = calculate_aic(params, dist, intervals)
        
        # C. åˆ†å¸ƒå‚æ•°è§£æä¸æ‰“å°
        if name == 'Exponential':
            scale = params[1]
            lambda_per_min = 1.0 / scale  # lambdaå•ä½ï¼šæ¬¡/åˆ†é’Ÿ
            lambda_per_hour = lambda_per_min * 60  # è½¬æ¢ä¸ºæ¬¡/å°æ—¶ï¼ˆæ’é˜Ÿè®ºå¸¸ç”¨ï¼‰
            param_str = f"Î»={lambda_per_min:.3f}/min (Î»={lambda_per_hour:.1f}/hr)"
            print(f"      æŒ‡æ•°åˆ†å¸ƒlambda: {param_str}")
        elif name == 'Gamma':
            shape, scale = params[0], params[2]
            param_str = f"Î±={shape:.2f}, Î²={scale:.2f}"
            print(f"      Gammaåˆ†å¸ƒå‚æ•°: å½¢çŠ¶Î±={shape:.2f}, å°ºåº¦Î²={scale:.2f}")
        elif name == 'Weibull':
            shape, scale = params[0], params[2]  # Weibull: shape(c), loc(0), scale(Î²)
            param_str = f"c={shape:.2f}, Î²={scale:.2f}"
            print(f"      Weibullåˆ†å¸ƒå‚æ•°: å½¢çŠ¶c={shape:.2f}, å°ºåº¦Î²={scale:.2f}")
        else:
            param_str = ""
        
        # D. K-S Test
        print(f"      è®¡ç®—K-Sæ£€éªŒ...")
        ks_stat, ks_p = stats.kstest(intervals, dist.name, args=params)
        print(f"      K-Sæ£€éªŒç»“æœ: ç»Ÿè®¡é‡={ks_stat:.4f}, på€¼={ks_p:.2e}")
        
        # E. Chi-Square Test
        print(f"      è®¡ç®—å¡æ–¹æ£€éªŒ...")
        chi2_stat, chi2_p = calculate_chi_square(intervals, dist, params)
        
        # F. Square Error (SE)
        print(f"      è®¡ç®—å¹³æ–¹è¯¯å·®...")
        sq_error = calculate_square_error(intervals, dist, params)
        
        # G. è®¡ç®—ç†è®ºPDFå€¼ï¼ˆä¿®å¤æ ¸å¿ƒï¼šè¡¥å……è¿™è¡Œï¼ï¼‰
        pdf_vals = dist.pdf(x_plot, *params)
        
        # ä¿å­˜ç»“æœ
        fit_results[name] = {
            'params': params,
            'aic': aic,
            'ks_stat': ks_stat, 'ks_p': ks_p,
            'chi2_stat': chi2_stat, 'chi2_p': chi2_p,
            'sq_error': sq_error
        }
        
        # è®°å½•æœ€ä¼˜AIC
        if aic < min_aic:
            min_aic = aic
            best_dist_name = name
        
        # H. ç»˜å›¾ (Weibullç”¨ç»¿è‰²ï¼Œä¿æŒé…è‰²åŒºåˆ†)
        if name == 'Exponential':
            line_color = 'black'
        elif name == 'Gamma':
            line_color = color
        elif name == 'Weibull':
            line_color = 'green'  # Weibullå›ºå®šä¸ºç»¿è‰²ï¼Œä¾¿äºåŒºåˆ†
        ax.plot(x_plot, pdf_vals, linestyle=style, 
                color=line_color, linewidth=2, label=f'{name} ({param_str})')
    
    # 3. è®¡ç®—Î”AICå¹¶æ‰“å°
    print(f"\n    ğŸ“Š {label}æ‹Ÿåˆç»“æœæ±‡æ€»:")
    for dist_name, metrics in fit_results.items():
        delta_aic = metrics['aic'] - min_aic
        fit_results[dist_name]['delta_aic'] = delta_aic
        print(f"      {dist_name}:")
        print(f"        - AIC={metrics['aic']:.1f}, Î”AIC={delta_aic:.1f}")
        print(f"        - K-S p={metrics['ks_p']:.2e}, Chi2 p={metrics['chi2_p']:.2e}")
        print(f"        - å¹³æ–¹è¯¯å·®={metrics['sq_error']:.6f}")
    
    print(f"      âœ… æœ€ä¼˜åˆ†å¸ƒ(AICæœ€å°): {best_dist_name} (AIC={min_aic:.1f})")
    
    # 4. ç”Ÿæˆå›¾ä¸Šçš„ç»Ÿè®¡æ–‡æœ¬ (çº¯è‹±æ–‡)
    stats_text = f"Sample N: {len(intervals)}\nMean: {np.mean(intervals):.2f} min\n"
    stats_text += "-" * 25 + "\n"
    for dist_name, metrics in fit_results.items():
        delta_aic = metrics['delta_aic']
        stats_text += f"[{dist_name}]\n"
        stats_text += f" AIC: {metrics['aic']:.1f} (Î”={delta_aic:.1f})\n"
        stats_text += f" SqErr: {metrics['sq_error']:.4f}\n"
        stats_text += f" K-S p: {metrics['ks_p']:.2e}\n"
        stats_text += f" Chi2 p: {metrics['chi2_p']:.2e}\n"
    
    # å›¾ç‰‡æ–‡æœ¬çº¯è‹±æ–‡ï¼Œä½ç½®è°ƒæ•´é¿å…é®æŒ¡
    ax.text(0.55, 0.15, stats_text, transform=ax.transAxes, fontsize=8,
            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.9), verticalalignment='top')

    # å›¾ç‰‡æ ‡é¢˜/æ ‡ç­¾çº¯è‹±æ–‡
    ax.set_title(f"{label} Arrival Intervals ({PEAK_START_HOUR}:00-{PEAK_END_HOUR}:00)", fontsize=14)
    ax.set_xlabel("Inter-arrival Time (minutes)", fontsize=12)
    ax.set_ylabel("Probability Density", fontsize=12)
    ax.legend(loc='upper right', fontsize=10)
    ax.set_xlim(0, 40)
    ax.grid(alpha=0.3)
    
    return fit_results

def step3_ultimate_fitting():
    print("="*60)
    print("å¼€å§‹æ‰§è¡Œ [Module 3] ç»ˆæåˆ†å¸ƒæ‹Ÿåˆ (å«Chi2 & SE & å…¨é‡æ‰“å°)")
    print("="*60)
    os.makedirs(FIGURE_DIR, exist_ok=True)
    
    # 1. åŠ è½½æ•°æ®
    df = load_and_preprocess_data()
    if df.empty: 
        print("âŒ æ— æ•°æ®ï¼Œç»ˆæ­¢ç¨‹åº")
        return

    # 2. åˆ†å‰²å·¥ä½œæ—¥/å‘¨æœ«å¹¶æå–é—´éš”
    print("\n" + "="*50)
    print("æå–é«˜å³°æœŸåˆ°è¾¾é—´éš”æ•°æ®")
    print("="*50)
    intervals_wd = get_pooled_inter_arrival_times(df[~df['is_weekend']], "å·¥ä½œæ—¥")
    intervals_we = get_pooled_inter_arrival_times(df[df['is_weekend']], "å‘¨æœ«")
    
    # 3. ç»˜å›¾æ‹Ÿåˆ (å›¾ç‰‡æ ‡ç­¾çº¯è‹±æ–‡)
    fig, axes = plt.subplots(1, 2, figsize=(18, 8))
    
    print("\n--- æ‹Ÿåˆå·¥ä½œæ—¥åˆ†å¸ƒ ---")
    res_wd = fit_and_compare_distributions(intervals_wd, "Weekday", "blue", axes[0])
    
    print("\n--- æ‹Ÿåˆå‘¨æœ«åˆ†å¸ƒ ---")
    res_we = fit_and_compare_distributions(intervals_we, "Weekend", "red", axes[1])
    
    # ä¿å­˜å›¾ç‰‡
    plt.tight_layout()
    save_path = os.path.join(FIGURE_DIR, 'distribution_fitting_comprehensive.png')
    plt.savefig(save_path, dpi=300)
    print(f"\nâœ… å›¾ç‰‡å·²ä¿å­˜è‡³: {save_path}")
    
    # 4. æ‰“å°æœ€ç»ˆæ±‡æ€»ï¼ˆæ§åˆ¶å°ç‰ˆï¼‰
    print("\n" + "="*60)
    print("æœ€ç»ˆæ‹Ÿåˆç»“æœæ±‡æ€» (æ§åˆ¶å°ç‰ˆ)")
    print("="*60)
    for label_cn, label_en, res in [("å·¥ä½œæ—¥", "Weekday", res_wd), ("å‘¨æœ«", "Weekend", res_we)]:
        print(f"\nğŸ“ˆ {label_cn} ({label_en}):")
        if res is None:
            print("  âŒ æ— æ‹Ÿåˆç»“æœ")
            continue
        for dist, metrics in res.items():
            delta_aic = metrics.get('delta_aic', 0)
            print(f"  ğŸ“Š {dist}:")
            print(f"    - å‚æ•°: {metrics['params']}")
            print(f"    - AIC={metrics['aic']:.1f}, Î”AIC={delta_aic:.1f}")
            print(f"    - K-Sæ£€éªŒ: ç»Ÿè®¡é‡={metrics['ks_stat']:.4f}, på€¼={metrics['ks_p']:.2e}")
            print(f"    - å¡æ–¹æ£€éªŒ: ç»Ÿè®¡é‡={metrics['chi2_stat']:.2f}, på€¼={metrics['chi2_p']:.2e}")
            print(f"    - å¹³æ–¹è¯¯å·®: {metrics['sq_error']:.6f}")
        # æ ‡æ³¨æœ€ä¼˜åˆ†å¸ƒ
        best_dist = min(res.keys(), key=lambda k: res[k]['aic'])
        print(f"  âœ… æœ€ä¼˜åˆ†å¸ƒ: {best_dist} (AICæœ€å°={res[best_dist]['aic']:.1f})")

if __name__ == "__main__":
    step3_ultimate_fitting()